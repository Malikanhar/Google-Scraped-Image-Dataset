{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6geLsIMhJrK"
      },
      "source": [
        "# Google Scraped Image Dataset\n",
        "\n",
        "From this [Kaggle dataset](https://www.kaggle.com/datasets/duttadebadri/image-classification), the problem is clear: we need to develop an image classification model to distinguish between traveling and photography images across four categories:\n",
        "\n",
        "1. Architecture\n",
        "\n",
        "2. Art and Culture\n",
        "\n",
        "3. Food and Drinks\n",
        "\n",
        "4. Travel and Adventure\n",
        "\n",
        "The goal is to train a robust model that can accurately classify images into these categories while maintaining efficiency in training and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJNu_FfEhGnn"
      },
      "source": [
        "## Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbccY1yYgMir",
        "outputId": "65f57084-dfdf-4399-bec3-c5c531c96569"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxxG642KhQ3D"
      },
      "source": [
        "Before downloading the dataset from Kaggle, we need to setup our Kaggle API key by generating and downloading it from https://www.kaggle.com/settings, then put the `kaggle.json` to current directory (in the same directory as this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwTMsw6QXtcy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nWNSB_UjSwi"
      },
      "source": [
        "## Download the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWOAspsRgSOu",
        "outputId": "5d3602e5-0305-4d34-c353-a57fd24f8960"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d duttadebadri/image-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVyO7dJKgYDa"
      },
      "outputs": [],
      "source": [
        "!unzip -q image-classification.zip -d image-classification-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnDwCUpZjV0y"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbEhvGLsivVD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "old_folder_name = \"./image-classification-dataset/images/images/food and d rinks\"\n",
        "new_folder_name = \"./image-classification-dataset/images/images/food\"\n",
        "\n",
        "os.rename(old_folder_name, new_folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byjxJchJj568"
      },
      "source": [
        "## Data Exploration and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycGk8ix4kOA3"
      },
      "source": [
        "### Data Distribution Across Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "aMgryClLj5eM",
        "outputId": "4f16c3e8-28b2-49f0-e41a-c828235a8fb3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to the dataset (training) directory\n",
        "dataset_path = \"./image-classification-dataset/images/images\"\n",
        "\n",
        "# Get class names (subfolder names)\n",
        "class_names = sorted(\n",
        "  [d for d in os.listdir(dataset_path)\n",
        "  if os.path.isdir(os.path.join(dataset_path, d))]\n",
        ")\n",
        "\n",
        "# Count the number of images in each class\n",
        "class_counts = {\n",
        "  cls: len(os.listdir(os.path.join(dataset_path, cls)))\n",
        "  for cls in class_names\n",
        "}\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color=\"skyblue\")\n",
        "plt.xlabel(\"Class Name\", fontsize=12)\n",
        "plt.ylabel(\"Number of Images\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"[Train] Data Distribution Across Classes\", fontsize=14)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvbSqJQDlFwm"
      },
      "source": [
        "Looks like we have a nearly balanced data distribution across classes, so a simple metric like accuracy can be used as our main metric in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "cYUKXDIQlY7a",
        "outputId": "8b0ac612-e912-4a2e-dc8a-101cf45b8722"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to the dataset (validation) directory\n",
        "dataset_path = \"./image-classification-dataset/validation/validation\"\n",
        "\n",
        "# Get class names (subfolder names)\n",
        "class_names = sorted(\n",
        "  [d for d in os.listdir(dataset_path)\n",
        "  if os.path.isdir(os.path.join(dataset_path, d))]\n",
        ")\n",
        "\n",
        "# Count the number of images in each class\n",
        "class_counts = {\n",
        "  cls: len(os.listdir(os.path.join(dataset_path, cls)))\n",
        "  for cls in class_names\n",
        "}\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color=\"skyblue\")\n",
        "plt.xlabel(\"Class Name\", fontsize=12)\n",
        "plt.ylabel(\"Number of Images\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"[Validation] Data Distribution Across Classes\", fontsize=14)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4nW3WPulj1F"
      },
      "source": [
        "However, since we only have a small validation sample, we will check whether accuracy alone is sufficient for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLktpuNnkVEt"
      },
      "source": [
        "### Image Quality Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5e7jjoMkc0S"
      },
      "source": [
        "Check the image blurriness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK-DpkKOkebq"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "def calculate_blurriness(image_path: str) -> tuple[str, float]:\n",
        "  \"\"\"Calculate blurriness using the Laplacian variance method.\"\"\"\n",
        "  image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "  if image is None:\n",
        "    return image_path, None\n",
        "  laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
        "  variance = laplacian.var()\n",
        "  return image_path, variance\n",
        "\n",
        "# Define the image folder\n",
        "dataset_directory = \"./image-classification-dataset/images/images\"\n",
        "image_folders = [\n",
        "  os.path.join(dataset_directory, image_folder)\n",
        "  for image_folder in os.listdir(dataset_directory)\n",
        "]\n",
        "\n",
        "# Get only 10% image samples for each classes\n",
        "image_path_samples = []\n",
        "for image_folder in image_folders:\n",
        "  image_paths = [\n",
        "    os.path.join(image_folder, f)\n",
        "    for f in os.listdir(image_folder)\n",
        "    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "  ]\n",
        "\n",
        "  # Define 10% sample size\n",
        "  sample_size = int(0.1 * len(image_paths))  # 10% of total images\n",
        "\n",
        "  # Randomly sample 10% of images\n",
        "  image_path_samples += random.sample(image_paths, sample_size)\n",
        "\n",
        "# Check the images' blurriness\n",
        "blurriness_results = [\n",
        "  calculate_blurriness(img_path) for img_path in image_path_samples\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "uhAI9dCUmg7y",
        "outputId": "a7200e51-dd10-424c-c42b-ee2b9894d970"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Extract blurriness scores (ignoring None values)\n",
        "blurriness_scores = [\n",
        "  score for _, score in blurriness_results if score is not None\n",
        "]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.hist(\n",
        "  blurriness_scores, bins=50, color='skyblue', edgecolor='black', alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"Blurriness Score (Laplacian Variance)\", fontsize=12)\n",
        "plt.ylabel(\"Number of Images\", fontsize=12)\n",
        "plt.title(\"Blurriness Distribution of Images\", fontsize=14)\n",
        "plt.axvline(\n",
        "  x=100, color='red', linestyle='dashed', label=\"Blurriness Threshold\"\n",
        ")\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuUsnUCZpKAs"
      },
      "source": [
        "Now, I would like to see the distribution for these blurriness categories:\n",
        "- Very Blurry: 0 - 50  \n",
        "- Blurry: 50 - 100  \n",
        "- Slightly Blurry: 100 - 500  \n",
        "- Sharp: 500 - 2000  \n",
        "- Very Sharp: 2,000 - 15,000  \n",
        "- Extreme (15,000+): > 15,000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "4dq2kfnUnNhN",
        "outputId": "b54c7c86-dc6f-46a3-b444-095164648e86"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "categories = {\n",
        "  \"Very Blurry (0-50)\": 0,\n",
        "  \"Blurry (50-100)\": 0,\n",
        "  \"Slightly Blurry (100-500)\": 0,\n",
        "  \"Sharp (500-2000)\": 0,\n",
        "  \"Very Sharp (2000-15000)\": 0,\n",
        "  \"Extreme (15K+)\": 0\n",
        "}\n",
        "\n",
        "for _, score in blurriness_results:\n",
        "  if score is None:\n",
        "    continue\n",
        "  if score < 50:\n",
        "    categories[\"Very Blurry (0-50)\"] += 1\n",
        "  elif score < 100:\n",
        "    categories[\"Blurry (50-100)\"] += 1\n",
        "  elif score < 500:\n",
        "    categories[\"Slightly Blurry (100-500)\"] += 1\n",
        "  elif score < 2000:\n",
        "    categories[\"Sharp (500-2000)\"] += 1\n",
        "  elif score < 15000:\n",
        "    categories[\"Very Sharp (2000-15000)\"] += 1\n",
        "  else:\n",
        "    categories[\"Extreme (15K+)\"] += 1\n",
        "\n",
        "# Plot bar\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(categories.keys(), categories.values(), color=[\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\"])\n",
        "plt.xlabel(\"Blurriness Type\", fontsize=12)\n",
        "plt.ylabel(\"Number of Images\", fontsize=12)\n",
        "plt.title(\"Image Blurriness Distribution\", fontsize=14)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7vwoBbyqKv9"
      },
      "source": [
        "Looks like we have a lot of sharp images and a small number of blurry ones. Most of the images can be used for the modeling phase. Now, let's randomly sample and plot images from each blurriness category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "dHaxXnb_n6_I",
        "outputId": "a7d835db-d701-43db-cff0-07e5964aaf34"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def get_sample_images(blurriness_results, num_samples=1):\n",
        "  categories = {\n",
        "    \"Very Blurry (0-50)\": [],\n",
        "    \"Blurry (50-100)\": [],\n",
        "    \"Slightly Blurry (100-500)\": [],\n",
        "    \"Sharp (500-2000)\": [],\n",
        "    \"Very Sharp (2000-15000)\": [],\n",
        "    \"Extreme (15K+)\": []\n",
        "  }\n",
        "\n",
        "  for img_path, score in blurriness_results:\n",
        "    if score is None:\n",
        "      continue\n",
        "    if score < 50:\n",
        "      categories[\"Very Blurry (0-50)\"].append(img_path)\n",
        "    elif score < 100:\n",
        "      categories[\"Blurry (50-100)\"].append(img_path)\n",
        "    elif score < 500:\n",
        "      categories[\"Slightly Blurry (100-500)\"].append(img_path)\n",
        "    elif score < 2000:\n",
        "      categories[\"Sharp (500-2000)\"].append(img_path)\n",
        "    elif score < 15000:\n",
        "      categories[\"Very Sharp (2000-15000)\"].append(img_path)\n",
        "    else:\n",
        "      categories[\"Extreme (15K+)\"].append(img_path)\n",
        "\n",
        "  sample_images = {\n",
        "    category: random.choice(paths)\n",
        "    if paths else None for category, paths in categories.items()\n",
        "  }\n",
        "  return sample_images\n",
        "\n",
        "sample_images = get_sample_images(blurriness_results)\n",
        "\n",
        "# Plot images\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for i, (category, img_path) in enumerate(sample_images.items()):\n",
        "  if img_path is None:\n",
        "    continue\n",
        "\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(category, fontsize=10)\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_3nQYxXkXG7"
      },
      "source": [
        "Check the overexposure and underexposure image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5DLco7zkfLk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def compute_brightness(image_path):\n",
        "  \"\"\"Calculate image brightness.\"\"\"\n",
        "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "  if img is None:\n",
        "    return None\n",
        "  return np.mean(img)\n",
        "\n",
        "exposure_results = [\n",
        "  (img_path, compute_brightness(img_path))\n",
        "  for img_path, _ in blurriness_results\n",
        "]\n",
        "exposure_results = [\n",
        "  (img_path, brightness) for img_path, brightness in exposure_results\n",
        "  if brightness is not None\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "lOIDtYTfryku",
        "outputId": "5623dc9d-67a2-4475-e33a-caedcabf0eca"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "categories = {\n",
        "    \"Underexposed (<50)\": [],\n",
        "    \"Normal (50-200)\": [],\n",
        "    \"Overexposed (>200)\": []\n",
        "}\n",
        "\n",
        "for img_path, brightness in exposure_results:\n",
        "  if brightness < 50:\n",
        "    categories[\"Underexposed (<50)\"].append(img_path)\n",
        "  elif brightness > 200:\n",
        "    categories[\"Overexposed (>200)\"].append(img_path)\n",
        "  else:\n",
        "    categories[\"Normal (50-200)\"].append(img_path)\n",
        "\n",
        "brightness_scores = [brightness for _, brightness in exposure_results]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(\n",
        "  brightness_scores, bins=50, color='skyblue', edgecolor='black', alpha=0.7\n",
        ")\n",
        "plt.xlabel(\"Brightness Level (Mean Pixel Value)\", fontsize=12)\n",
        "plt.ylabel(\"Number of Images\", fontsize=12)\n",
        "plt.title(\"Image Exposure Distribution\", fontsize=14)\n",
        "plt.axvline(\n",
        "  x=50, color='red', linestyle='dashed', label=\"Underexposed Threshold\"\n",
        ")\n",
        "plt.axvline(\n",
        "  x=200, color='red', linestyle='dashed', label=\"Overexposed Threshold\"\n",
        ")\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kahYq5gLsLBE"
      },
      "source": [
        "We have some under-exposure images, let's visualize these images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "muyPw_3OsKXa",
        "outputId": "89d80079-0969-4df0-e671-390eea8802ba"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "sample_images = {\n",
        "  category: random.choice(paths) if paths else None\n",
        "  for category, paths in categories.items()\n",
        "}\n",
        "\n",
        "# Plot images\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "for i, (category, img_path) in enumerate(sample_images.items()):\n",
        "  if img_path is None:\n",
        "    continue\n",
        "\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  plt.subplot(1, 3, i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(category, fontsize=10)\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l35J_mimsYo8"
      },
      "source": [
        "For these under- and overexposed images, we can fix them by applying image processing techniques. But for now, let's see how the model handles this data. Later, we will correct these images and evaluate the significance of the improvement made by addressing this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV9t2a8YswOl"
      },
      "source": [
        "### Check the Image Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJBYAN4usyvk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def get_image_size(image_path):\n",
        "  img = cv2.imread(image_path)\n",
        "  if img is None:\n",
        "    return None  # Skip unreadable images\n",
        "  return img.shape[1], img.shape[0]  # (width, height)\n",
        "\n",
        "# Process all images\n",
        "sampled_images = [(img_path, get_image_size(img_path)) for img_path, _ in blurriness_results]\n",
        "\n",
        "# Remove None values (unreadable images)\n",
        "sampled_images = [(img_path, size) for img_path, size in sampled_images if size is not None]\n",
        "\n",
        "# Extract width and height lists\n",
        "widths = [size[0] for _, size in sampled_images]\n",
        "heights = [size[1] for _, size in sampled_images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "daVSlQ1uGEXE",
        "outputId": "7595ff5b-8267-4536-d9d0-182667f31fe9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Width distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(widths, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel(\"Image Width\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Distribution of Image Widths\")\n",
        "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Height distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(heights, bins=50, color='green', alpha=0.7, edgecolor='black')\n",
        "plt.xlabel(\"Image Height\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Distribution of Image Heights\")\n",
        "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMDyPceTGH08",
        "outputId": "2db6888c-d0f8-4542-9786-f1625d8616c9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count occurrences of each image size\n",
        "size_counts = Counter(size for _, size in sampled_images)\n",
        "\n",
        "# Display the most common image sizes\n",
        "print(\"Most common image sizes:\")\n",
        "for size, count in size_counts.most_common(10):\n",
        "  print(f\"{size}: {count} images\")\n",
        "\n",
        "# Extract file paths of the smallest and largest images\n",
        "smallest_img_path, smallest_img_size = min(sampled_images, key=lambda x: x[1][0] * x[1][1])\n",
        "largest_img_path, largest_img_size = max(sampled_images, key=lambda x: x[1][0] * x[1][1])\n",
        "\n",
        "print(f\"\\nSmallest image size: {smallest_img_size}\")\n",
        "print(f\"Largest image size: {largest_img_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "4X82m3JNGsq-",
        "outputId": "05092f75-f7bf-4e54-a5bd-583025163346"
      },
      "outputs": [],
      "source": [
        "def display_image(image_path, title):\n",
        "  img = cv2.imread(image_path)\n",
        "  if img is not None:\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "  else:\n",
        "    print(f\"Could not load image: {image_path}\")\n",
        "\n",
        "# Plot images\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "display_image(smallest_img_path, f\"Smallest Image: {smallest_img_size}\")\n",
        "plt.subplot(1, 2, 2)\n",
        "display_image(largest_img_path, f\"Largest Image: {largest_img_size}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDYQzCniHTL0"
      },
      "source": [
        "From the dataset, we can see that most images are small to medium in size (ranging from 128x128 to 512x512). Knowing this, it suggests that using a deeper model may not provide significant improvement but will instead increase training and deployment costs due to its complexity. Instead, a small to medium-sized model should be more sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ZJVelTkJaW"
      },
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "The only data preprocessing I applied is standardizing the image size to 512x512. Reducing the size to 128x128 may lead to a significant loss of details and limitations in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ1VKrKPJkzX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Target size\n",
        "TARGET_SIZE = 512\n",
        "\n",
        "def resize_with_padding(image, target_size):\n",
        "  \"\"\"Resize image with padding to maintain aspect ratio.\"\"\"\n",
        "  h, w = image.shape[:2]\n",
        "\n",
        "  # Compute scaling factor\n",
        "  scale = target_size / max(h, w)\n",
        "  new_w, new_h = int(w * scale), int(h * scale)\n",
        "\n",
        "  # Resize image\n",
        "  resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "  # Create a blank canvas\n",
        "  padded = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
        "\n",
        "  # Compute padding offsets\n",
        "  top = (target_size - new_h) // 2\n",
        "  left = (target_size - new_w) // 2\n",
        "\n",
        "  # Place resized image on canvas\n",
        "  padded[top:top+new_h, left:left+new_w] = resized\n",
        "\n",
        "  return padded\n",
        "\n",
        "def preprocess_data(input_folder, output_folder, target_size=TARGET_SIZE):\n",
        "  \"\"\"Pre-process the input folder by resizing it to a predefined target size.\"\"\"\n",
        "  # Ensure output directory exists\n",
        "  os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "  # Process all images in the folder\n",
        "  for image_dir in os.listdir(input_folder):\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = os.path.join(output_folder, image_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for filename in tqdm(\n",
        "      os.listdir(os.path.join(input_folder, image_dir)),\n",
        "      f\"Processing {image_dir}\"\n",
        "    ):\n",
        "      if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        input_path = os.path.join(input_folder, image_dir, filename)\n",
        "\n",
        "        image = cv2.imread(input_path)\n",
        "        if image is None:\n",
        "          print(f\"Skipping unreadable image: {filename}\")\n",
        "          continue\n",
        "\n",
        "        # Save the processed image\n",
        "        output_path = os.path.join(output_folder, image_dir, filename)\n",
        "        resized_image = resize_with_padding(image, target_size=target_size)\n",
        "        cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "  print(\"Resizing complete! Images saved in:\", output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Mxt6FFY8Xl"
      },
      "source": [
        "Process Training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fb1kDgMYyly",
        "outputId": "a4f5e662-e0e3-4966-bd27-1a104c486c0d"
      },
      "outputs": [],
      "source": [
        "# Input and output paths\n",
        "input_folder = \"./image-classification-dataset/images/images\"\n",
        "output_folder = \"./processed-dataset/images\"\n",
        "\n",
        "preprocess_data(input_folder=input_folder, output_folder=output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_khN10b5Y-Ik"
      },
      "source": [
        "Process Validation images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SKtM2NgY-c5",
        "outputId": "71758241-dee0-4a2f-c028-0e1c55963fb8"
      },
      "outputs": [],
      "source": [
        "# Input and output paths\n",
        "input_folder = \"./image-classification-dataset/validation/validation\"\n",
        "output_folder = \"./processed-dataset/validation\"\n",
        "\n",
        "preprocess_data(input_folder=input_folder, output_folder=output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "whvF0H5dL_m-",
        "outputId": "2c39130f-4351-4ffa-c546-231f11ac2c8d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "output_folder = \"./processed-dataset/images\"\n",
        "\n",
        "processed_images = [\n",
        "  os.path.join(output_folder, image_dir, f)\n",
        "  for image_dir in os.listdir(output_folder)\n",
        "  if os.path.isdir(os.path.join(output_folder, image_dir))\n",
        "  for f in os.listdir(os.path.join(output_folder, image_dir))\n",
        "  if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "]\n",
        "\n",
        "# Randomly sample images for visualization\n",
        "num_samples = min(5, len(processed_images))\n",
        "sampled_images = random.sample(processed_images, num_samples)\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, img_path in enumerate(sampled_images):\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  plt.subplot(1, num_samples, i + 1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(f\"{os.path.basename(img_path)}\")\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn2po8WVN9qS"
      },
      "source": [
        "For now, I believe our data preprocessing is sufficient. The model should be able to handle slightly blurry images as well as under and overexposed images. Next, I will proceed with the modeling phase. If some models, including deeper ones, struggle to achieve good performance, then refining the data will be the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6vN_hTHkLJz"
      },
      "source": [
        "## Modelling\n",
        "\n",
        "Looking back at the problem, the goal is to classify travel and photography pictures. Having a small yet powerful model is a significant advantage. One model I want to try is MobileNetV4 -- an efficient, lightweight model that offers high accuracy, fast training, and low deployment costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ1HGVfxjm_5"
      },
      "source": [
        "### Vanilla Model -- MobilenetV4 Small, No Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_7a6omKbNsC",
        "outputId": "68182a6a-1eea-4c17-c685-5b0673a560f7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data directories\n",
        "train_dir = \"./processed-dataset/images\"\n",
        "val_dir = \"./processed-dataset/validation\"\n",
        "\n",
        "# Define transformations (Resize to 224x224 and Normalize)\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize((224, 224)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962,
          "referenced_widgets": [
            "24d58508109f4fe796eb53ecebb2fef5",
            "41e5601cc163449dad57c3183398f5c6",
            "919df7b618db475babae0f1144d4ce35",
            "b3c2b344fd1c49b689b101ab6192f833",
            "eca372d31a254c94a71fa10c37a553f6",
            "8ff9b12f26c142eda668306bc58398c3",
            "f76e051c50c64cb885da36dbbbaf6958",
            "951099e686674bc4967bf023d0b18a9d",
            "fd684996851e4ba291f99481c53ea763",
            "d6317a30a5064c3cb854fbbeb16b7475",
            "dcfda6a281c9435b8bd3bf0364504add"
          ]
        },
        "id": "kDbWKY8akL3R",
        "outputId": "8601c38a-f183-433d-b376-a2275c91df3a"
      },
      "outputs": [],
      "source": [
        "# Load MobileNetV4 Small from timm\n",
        "model = timm.create_model(\"mobilenetv4_conv_small.e2400_r224_in1k\", pretrained=True, num_classes=num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Metric storage\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"\\n[Epoch {epoch+1}/{num_epochs}]\")\n",
        "\n",
        "  # -------------------- Training --------------------\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "\n",
        "  for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    train_total += labels.size(0)\n",
        "    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  train_epoch_loss = train_loss / len(train_loader)\n",
        "  train_epoch_acc = train_correct / train_total\n",
        "  train_losses.append(train_epoch_loss)\n",
        "  train_accuracies.append(train_epoch_acc)\n",
        "\n",
        "  print(f\"Train Loss: {train_epoch_loss:.4f}, Accuracy: {train_epoch_acc:.4f}\")\n",
        "\n",
        "  # -------------------- Validation --------------------\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  val_epoch_loss = val_loss / len(val_loader)\n",
        "  val_epoch_acc = val_correct / val_total\n",
        "  val_losses.append(val_epoch_loss)\n",
        "  val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "  print(f\"Validation Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}\")\n",
        "\n",
        "  # Save the best model based on validation accuracy\n",
        "  if val_epoch_acc > best_val_acc:\n",
        "    best_val_acc = val_epoch_acc\n",
        "    torch.save(model.state_dict(), \"mobilenetv4_best.pth\")\n",
        "    print(\"Best model saved!\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Store the training metrics\n",
        "training_results = []\n",
        "training_results.append({\n",
        "      \"model_name\": \"mobilenetv4_conv_small.e2400_r224_in1k\",\n",
        "      \"train_losses\": train_losses,\n",
        "      \"train_accuracies\": train_accuracies,\n",
        "      \"val_losses\": val_losses,\n",
        "      \"val_accuracies\": val_accuracies\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIFSxB3cj4Y7"
      },
      "source": [
        "### With Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIvsvj-cj4IP",
        "outputId": "c28289cd-6fb7-46f1-a920-d7a1157f6037"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data directories\n",
        "train_dir = \"./processed-dataset/images\"\n",
        "val_dir = \"./processed-dataset/validation\"\n",
        "\n",
        "# -------------------- Data Augmentation --------------------\n",
        "train_transform_aug = transforms.Compose([\n",
        "  transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop\n",
        "  transforms.RandomRotation(20),  # Rotate ±20 degrees\n",
        "  transforms.RandomHorizontalFlip(0.5),  # 50% chance to flip\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation transform (same for fair comparison)\n",
        "val_transform = transforms.Compose([\n",
        "  transforms.Resize((224, 224)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset_aug = datasets.ImageFolder(root=train_dir, transform=train_transform_aug)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(train_dataset_aug.classes)\n",
        "\n",
        "# Load MobileNetV4 Small from timm\n",
        "model = timm.create_model(\"mobilenetv4_conv_small.e2400_r224_in1k\", pretrained=True, num_classes=num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Metric storage\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "best_val_acc = 0.0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"\\n[Epoch {epoch+1}/{num_epochs}]\")\n",
        "\n",
        "  # -------------------- Training --------------------\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    train_total += labels.size(0)\n",
        "    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  train_epoch_loss = train_loss / len(train_loader)\n",
        "  train_epoch_acc = train_correct / train_total\n",
        "  train_losses.append(train_epoch_loss)\n",
        "  train_accuracies.append(train_epoch_acc)\n",
        "\n",
        "  print(f\"Train Loss: {train_epoch_loss:.4f}, Accuracy: {train_epoch_acc:.4f}\")\n",
        "\n",
        "  # -------------------- Validation --------------------\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  val_epoch_loss = val_loss / len(val_loader)\n",
        "  val_epoch_acc = val_correct / val_total\n",
        "  val_losses.append(val_epoch_loss)\n",
        "  val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "  print(f\"Validation Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}\")\n",
        "\n",
        "  # Save the best model\n",
        "  if val_epoch_acc > best_val_acc:\n",
        "    best_val_acc = val_epoch_acc\n",
        "    torch.save(model.state_dict(), \"mobilenetv4_aug_best.pth\")\n",
        "    print(\"Best model saved!\")\n",
        "\n",
        "training_results.append({\n",
        "  \"model_name\": \"mobilenetv4_conv_small.e2400_r224_in1k-data_augment\",\n",
        "  \"train_losses\": train_losses,\n",
        "  \"train_accuracies\": train_accuracies,\n",
        "  \"val_losses\": val_losses,\n",
        "  \"val_accuracies\": val_accuracies\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGQGH0tjmgYi"
      },
      "source": [
        "### Larger Model -- MobilenetV4 Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567,
          "referenced_widgets": [
            "8fbdff3fcd724c6da72d3757c73816d9",
            "bb3cc8621afe48ddb3c200b1bbe8cae3",
            "5b04990296ba41b3a5e506c9e8991e59",
            "cd44b74524af45f5922f6ca37765b0fd",
            "adddb43b8d214427b45920db2006c6c9",
            "d749583ce96242e2ab74e9a7bca36dd1",
            "f7d1d0504e1c404d8d52c08196f1d0a2",
            "52bcdf6c758e42b0a515fe249c55cd1f",
            "70be595ce98e4e65bba1013c069eb568",
            "25cf9b6aaeb34971b21acbcc515ca131",
            "bb797b80408446a2858a6bd55fc97e5f"
          ]
        },
        "id": "GhgUFMEKmLIv",
        "outputId": "a79512d3-9181-496c-b64d-faf950486d34"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data directories\n",
        "train_dir = \"./processed-dataset/images\"\n",
        "val_dir = \"./processed-dataset/validation\"\n",
        "\n",
        "# -------------------- Data Augmentation --------------------\n",
        "train_transform_aug = transforms.Compose([\n",
        "  transforms.RandomResizedCrop(384, scale=(0.8, 1.0)),  # Random crop\n",
        "  transforms.RandomRotation(20),  # Rotate ±20 degrees\n",
        "  transforms.RandomHorizontalFlip(0.5),  # 50% chance to flip\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation transform (same for fair comparison)\n",
        "val_transform = transforms.Compose([\n",
        "  transforms.Resize((384, 384)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset_aug = datasets.ImageFolder(root=train_dir, transform=train_transform_aug)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(train_dataset_aug.classes)\n",
        "\n",
        "# Load MobileNetV4 Small from timm\n",
        "model = timm.create_model(\"mobilenetv4_conv_large.e600_r384_in1k\", pretrained=True, num_classes=num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Metric storage\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "best_val_acc = 0.0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"\\n[Epoch {epoch+1}/{num_epochs}]\")\n",
        "\n",
        "  # -------------------- Training --------------------\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    train_total += labels.size(0)\n",
        "    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  train_epoch_loss = train_loss / len(train_loader)\n",
        "  train_epoch_acc = train_correct / train_total\n",
        "  train_losses.append(train_epoch_loss)\n",
        "  train_accuracies.append(train_epoch_acc)\n",
        "\n",
        "  print(f\"Train Loss: {train_epoch_loss:.4f}, Accuracy: {train_epoch_acc:.4f}\")\n",
        "\n",
        "  # -------------------- Validation --------------------\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  val_epoch_loss = val_loss / len(val_loader)\n",
        "  val_epoch_acc = val_correct / val_total\n",
        "  val_losses.append(val_epoch_loss)\n",
        "  val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "  print(f\"Validation Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}\")\n",
        "\n",
        "  # Save the best model\n",
        "  if val_epoch_acc > best_val_acc:\n",
        "    best_val_acc = val_epoch_acc\n",
        "    torch.save(model.state_dict(), \"mobilenetv4-large_aug_best.pth\")\n",
        "    print(\"Best model saved!\")\n",
        "\n",
        "training_results.append({\n",
        "  \"model_name\": \"mobilenetv4_conv_large.e600_r384_in1k-data_augment\",\n",
        "  \"train_losses\": train_losses,\n",
        "  \"train_accuracies\": train_accuracies,\n",
        "  \"val_losses\": val_losses,\n",
        "  \"val_accuracies\": val_accuracies\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swufYlMArO6n"
      },
      "source": [
        "### Visualize Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z3CXC04rPcb"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(training_results[0][\"train_losses\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# -------------------- Loss Comparison --------------------\n",
        "plt.subplot(1, 2, 1)\n",
        "for res in training_results[:2]:\n",
        "    plt.plot(epochs, res[\"train_losses\"], label=f\"{res['model_name']} - Train\", linestyle=\"--\", marker=\"o\")\n",
        "    plt.plot(epochs, res[\"val_losses\"], label=f\"{res['model_name']} - Val\", linestyle=\"-\", marker=\"s\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training & Validation Loss Comparison\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# -------------------- Accuracy Comparison --------------------\n",
        "plt.subplot(1, 2, 2)\n",
        "for res in training_results[:2]:\n",
        "    plt.plot(epochs, res[\"train_accuracies\"], label=f\"{res['model_name']} - Train\", linestyle=\"--\", marker=\"o\")\n",
        "    plt.plot(epochs, res[\"val_accuracies\"], label=f\"{res['model_name']} - Val\", linestyle=\"-\", marker=\"s\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training & Validation Accuracy Comparison\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WNbOqqf8lHu"
      },
      "outputs": [],
      "source": [
        "for res in training_results:\n",
        "  best_idx = np.argmax(res.get('val_accuracies'))\n",
        "  print(f\"Result for {res.get('model_name')}\")\n",
        "  print(f\"Train accuracy: {res.get('train_accuracies')[best_idx]}\")\n",
        "  print(f\"Val accuracy: {res.get('val_accuracies')[best_idx]}\")\n",
        "  print(f\"Train loss: {res.get('train_losses')[best_idx]}\")\n",
        "  print(f\"Val loss: {res.get('val_losses')[best_idx]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC2ySwjKppoG"
      },
      "source": [
        "## Final Verdict on Image Classification Exercise\n",
        "\n",
        "A small model (MobileNetV4-small) achieved ~88% validation accuracy with data augmentation. This suggests that data augmentation effectively helps the model learn unseen features from the training data, improving its ability to generalize and accurately classify images in the validation set.\n",
        "The larger model requires more training time to achieve good results. It seems that 10 epochs are not sufficient, and additional training epochs may be needed for better convergence and performance.\n",
        "\n",
        "**Improvement Possibility**\n",
        "- **Model-Centric Approach**: Training the model for more epochs could help it converge better. Additionally, experimenting with a deeper model might improve accuracy. However, to prevent overfitting, techniques such as dropout, weight regularization, or data augmentation should be applied. Fine-tuning a pre-trained model on our dataset can also be an effective way to boost performance while maintaining efficiency.  \n",
        "\n",
        "- **Data-Centric Approach**: Improving image quality can significantly enhance model performance. This can be done by omitting excessively blurry images, correcting under- and overexposed images through image processing techniques, and ensuring a more balanced and diverse dataset. Applying advanced augmentation techniques, such as adaptive histogram equalization or noise reduction, can also help the model learn more robust features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMFvOjcA9Ttb"
      },
      "source": [
        "## Run on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRgHVYry9S18"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Target size\n",
        "TARGET_SIZE = 512\n",
        "\n",
        "def resize_with_padding(image, target_size):\n",
        "  \"\"\"Resize image with padding to maintain aspect ratio.\"\"\"\n",
        "  h, w = image.shape[:2]\n",
        "\n",
        "  # Compute scaling factor\n",
        "  scale = target_size / max(h, w)\n",
        "  new_w, new_h = int(w * scale), int(h * scale)\n",
        "\n",
        "  # Resize image\n",
        "  resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "  # Create a blank canvas\n",
        "  padded = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
        "\n",
        "  # Compute padding offsets\n",
        "  top = (target_size - new_h) // 2\n",
        "  left = (target_size - new_w) // 2\n",
        "\n",
        "  # Place resized image on canvas\n",
        "  padded[top:top+new_h, left:left+new_w] = resized\n",
        "\n",
        "  return padded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhzNrs2W9cFp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "\n",
        "transform = transforms.Compose([\n",
        "  transforms.Lambda(lambda img: resize_with_padding(img, target_size=224)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "classes = {\n",
        "  0: \"Art & Culture\",\n",
        "  1: \"Architecture\",\n",
        "  2: \"Food and Drinks\",\n",
        "  3: \"Travel and Adventure\"\n",
        "}\n",
        "\n",
        "image_dir = \"./image-classification-dataset/test/test/classify\"\n",
        "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "image_paths.sort()\n",
        "\n",
        "model = timm.create_model(\"mobilenetv4_conv_small.e2400_r224_in1k\", pretrained=True, num_classes=4)\n",
        "model.load_state_dict(torch.load(\"mobilenetv4_best.pth\", map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "prediction_results = {\n",
        "  \"filename\": [],\n",
        "  \"predicted\": [],\n",
        "  \"confidence\": []\n",
        "}\n",
        "\n",
        "# Process and run inference\n",
        "for img_path in image_paths:\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  input_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "    conf, predicted = torch.max(probabilities, 1)\n",
        "\n",
        "  predicted.item()  # Convert tensor to integer\n",
        "  prediction_results[\"filename\"].append(os.path.basename(img_path))\n",
        "  prediction_results[\"predicted\"].append(classes[predicted.item()])\n",
        "  prediction_results[\"confidence\"].append(conf.item()*100)\n",
        "\n",
        "df = pd.DataFrame(prediction_results)\n",
        "df.to_csv(\"image_classification_prediction.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DJNu_FfEhGnn",
        "5nWNSB_UjSwi",
        "byjxJchJj568",
        "F2ZJVelTkJaW",
        "T6vN_hTHkLJz",
        "oMFvOjcA9Ttb"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jupyter-py3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24d58508109f4fe796eb53ecebb2fef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41e5601cc163449dad57c3183398f5c6",
              "IPY_MODEL_919df7b618db475babae0f1144d4ce35",
              "IPY_MODEL_b3c2b344fd1c49b689b101ab6192f833"
            ],
            "layout": "IPY_MODEL_eca372d31a254c94a71fa10c37a553f6"
          }
        },
        "25cf9b6aaeb34971b21acbcc515ca131": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e5601cc163449dad57c3183398f5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ff9b12f26c142eda668306bc58398c3",
            "placeholder": "​",
            "style": "IPY_MODEL_f76e051c50c64cb885da36dbbbaf6958",
            "value": "model.safetensors: 100%"
          }
        },
        "52bcdf6c758e42b0a515fe249c55cd1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b04990296ba41b3a5e506c9e8991e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52bcdf6c758e42b0a515fe249c55cd1f",
            "max": 131022456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70be595ce98e4e65bba1013c069eb568",
            "value": 131022456
          }
        },
        "70be595ce98e4e65bba1013c069eb568": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fbdff3fcd724c6da72d3757c73816d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb3cc8621afe48ddb3c200b1bbe8cae3",
              "IPY_MODEL_5b04990296ba41b3a5e506c9e8991e59",
              "IPY_MODEL_cd44b74524af45f5922f6ca37765b0fd"
            ],
            "layout": "IPY_MODEL_adddb43b8d214427b45920db2006c6c9"
          }
        },
        "8ff9b12f26c142eda668306bc58398c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919df7b618db475babae0f1144d4ce35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951099e686674bc4967bf023d0b18a9d",
            "max": 15223016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd684996851e4ba291f99481c53ea763",
            "value": 15223016
          }
        },
        "951099e686674bc4967bf023d0b18a9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adddb43b8d214427b45920db2006c6c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c2b344fd1c49b689b101ab6192f833": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6317a30a5064c3cb854fbbeb16b7475",
            "placeholder": "​",
            "style": "IPY_MODEL_dcfda6a281c9435b8bd3bf0364504add",
            "value": " 15.2M/15.2M [00:00&lt;00:00, 82.5MB/s]"
          }
        },
        "bb3cc8621afe48ddb3c200b1bbe8cae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d749583ce96242e2ab74e9a7bca36dd1",
            "placeholder": "​",
            "style": "IPY_MODEL_f7d1d0504e1c404d8d52c08196f1d0a2",
            "value": "model.safetensors: 100%"
          }
        },
        "bb797b80408446a2858a6bd55fc97e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd44b74524af45f5922f6ca37765b0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25cf9b6aaeb34971b21acbcc515ca131",
            "placeholder": "​",
            "style": "IPY_MODEL_bb797b80408446a2858a6bd55fc97e5f",
            "value": " 131M/131M [00:00&lt;00:00, 187MB/s]"
          }
        },
        "d6317a30a5064c3cb854fbbeb16b7475": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d749583ce96242e2ab74e9a7bca36dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfda6a281c9435b8bd3bf0364504add": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eca372d31a254c94a71fa10c37a553f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f76e051c50c64cb885da36dbbbaf6958": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7d1d0504e1c404d8d52c08196f1d0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd684996851e4ba291f99481c53ea763": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
